{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "337d45ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c12c879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b582f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a006c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40761ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1961021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e55e3ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/hashstudioz/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a668e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting text from any article on the web\n",
    "\n",
    "class WebScraper:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        \n",
    "    def extract_arcticle_text(self):\n",
    "        response = requests.get(self.url)\n",
    "        html_content = response.content\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        article_text = soup.get_text()\n",
    "        return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3fc8af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "924a8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and preprocess the text extracted from the article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9365df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    \n",
    "    def __init__(self, nltk_stopwords):\n",
    "        self.nltk_stopwords = nltk_stopwords\n",
    "        \n",
    "    def tokenize_and_clean(self, text):\n",
    "        words = text.split()\n",
    "        filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in self.nltk_stopwords]\n",
    "        return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a786eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f65271e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The entire ETL (Extract, Transform, Load) process for extracting article text, processing it, and generating \n",
    "# a DataFrame of word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "091eda45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ETLPipeline:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.nltk_stopwords = set(stopwords.words('english'))\n",
    "        \n",
    "    def run(self):\n",
    "        scrapper = WebScraper(self.url)\n",
    "        article_text = scrapper.extract_arcticle_text()\n",
    "        \n",
    "        processor = TextProcessor(self.nltk_stopwords)\n",
    "        filtered_words = processor.tokenize_and_clean(article_text)\n",
    "        \n",
    "        word_freq = Counter(filtered_words)\n",
    "        df = pd.DataFrame(word_freq.items(), columns=['Words', 'Total_Count'])\n",
    "#         df = df.sort_values(by='Total_Count', ascending=False)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f81e3d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Words', 'Total_Count']\n",
      "         Words  Total_Count\n",
      "0          web           15\n",
      "1         data           32\n",
      "2          etl           14\n",
      "3     pipeline           14\n",
      "4        using           10\n",
      "..         ...          ...\n",
      "198   facebook            1\n",
      "199  instagram            1\n",
      "200     medium            1\n",
      "201   linkedin            1\n",
      "202  copyright            1\n",
      "\n",
      "[203 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     article_url = \"https://amankharwal.medium.com/what-is-time-series-analysis-in-data-science-f89aaa1c0814\"\n",
    "    article_url = \"https://thecleverprogrammer.com/2023/08/14/web-data-etl-pipeline-using-python/\"\n",
    "    pipeline = ETLPipeline(article_url)\n",
    "    result_df = pipeline.run()\n",
    "    filename = \"Article_Scraping.csv\"\n",
    "    result_df.to_csv(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693ec9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
